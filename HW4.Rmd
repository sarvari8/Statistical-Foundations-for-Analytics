---
header-includes:
- \usepackage{amssymb, amsmath, amsthm}
- \usepackage{tabu}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\var}{{\rm Var}}
- \newcommand{\N}{\mathcal{N}}
output: pdf_document
---

\noindent \begin{tabu} to \textwidth {@{}X[4 l] @{}X[r]}
  \textbf{Homework 4}           & \\ 
  \textbf{MSBA 400: Statistical Foundations for Data Analytics}   & \\ 
  \textbf{UID 106082225, Sarvari Pidaparty}         & 
\end{tabu}

\bigskip
### Question: Prediction of Catalogue Orders

The dataset `cat_buy.rda` contains data on the response of customers to the mailing of spring catalogues.  The variable `buytabw` is `1` if there is an order from this spring catalogue and `0` if not.  This is the dependent or response variable (literally was there a "response" to or order from the direct mailing).  

This spring catalogue was called a "tabloid" in the industry. The catalogue featured women's clothing and shoes. The independent variables represent information gathered from the internal `house file` of the past order activity of these 20,617 customers who received this catalogue.  

In direct marketing, the predictor variables are typically of the "RFM" type: 1. Recency 2. Frequency and 3. Monetary value. This data set has both information on the volume of past orders as well as the recency of these orders. 

The variables are:
* tabordrs  (total orders from past tabloids)  
* divsords  (total orders of shoes in past)  
* divwords  (total orders of women's clothes in past)  
* spgtabord (total orders from past spring cats)  
* moslsdvs  (mos since last shoe order)  
* moslsdvw  (mos since last women's clothes order)  
* moslstab  (mos since last tabloid order)  
* orders    (total orders) 

### part A

Use the R `sample` command to randomly sample 1/2 of the data.  The sample command will sample randomly from a list of numbers, e.g. `r sample(1:10,size=5)` will select 5 from the numbers 1,2,3,4,5,6,7,8,9,10.  

Use `sample` to select row numbers and then use these row numbers to divide your data into two parts. One part for estimation and one part for validation.

Hint: see code below (modify)

```{r}
load("~/Documents/Fall 2022/Statistical Foundations/cat_buy.rda")
count = nrow(cat_buy)
ind.est=sample(1:count, size = count/2)
est_sample = cat_buy[ind.est,]      
holdout_sample = cat_buy[-ind.est,]

head(est_sample) #print out head for better understanding
```
```{r}
head(holdout_sample)
```

### part B

Fit a logistic regression model using the estimation sample produced in part A. Eliminate insignificant variables.

Discuss your final specification, do the signs of the coefficients make sense to you?

Should you worry about multi-colinearity in this dataset?

```{r}
out_model = glm(buytabw ~ ., family = "binomial", data = est_sample)
summary(out_model)
```

The variable `divsords` can be eliminated, as p = 0.845, which is greater than the significance level $\alpha = 0.05$. We fit the model again as below eliminating `divsords`.

```{r}
out_model1 = glm(buytabw ~ tabordrs + divwords + spgtabord + moslsdvs + moslsdvw + moslstab + orders , family = "binomial", data = est_sample)
summary(out_model1)
```

The intercept coefficient is the log-odds value of the dependent variable when all independent variables are zero. 
The other coefficients describe how the log-odds of `buytabw` change when the corresponding variable (X) increases by 1 unit (since all X's here are numeric and not categorical). If you exponentiate the coefficients, you get the odds of `buytabw` $\frac{p}{1-p}$. 

The positive coefficients against `tabordrs`, `divwords` and `spgtabord` indicate that an increase in any of these predictors results in an increased probability of orders from the catalogue. The negative coefficients against `moslsdvs`, `moslsdvw`, `moslstab` and `orders` indicate that an increase in any of these predictors results in a decreased probability of orders from the catalogue.

```{r}
cor(est_sample)
```

There may be an issue of multicolinearity here as we can see a few values close to 1. For example, `spgtabord` and `tabordrs` have a correlation coefficient of ~0.89 which leads to a multicolinearity issue.

Function VIF can be used to understand this as well:

vif(out_model1)
```{r echo=FALSE, out.width='60%'}
knitr::include_graphics('/Users/sarvaripidaparty/Desktop/VIF.png')
```

### part C

Use the best-fit from part B to predict using the holdout sample.   

Plot boxplots of the fitted probabilities for each value of `buytabw` for the holdout sample (see code snippets from Chapter 7 for an example) 
```{r}
library(ggplot2)
phat = predict(out_model1, holdout_sample, type="response")
qplot(factor(holdout_sample$buytabw), phat, geom="boxplot", fill=I("green"),xlab="buytabw") +
    theme(axis.title=element_text(size=rel(1.5)),
        axis.text=element_text(size=rel(1.25),colour=I("red")))
```

There is an overlap between the two categories, which is not ideal.

Compute a "lift" table as done in Chapter 7 code snippets.
```{r}
deciles = cut(phat, breaks = quantile(phat, probs = c(seq(from=0,to=1,by=.1))), include.lowest=TRUE)
deciles = as.numeric(deciles)
```


```{r}
df = data.frame(deciles = deciles, phat=phat, default = holdout_sample$buytabw)
lift = aggregate(df,by=list(deciles), FUN="mean", data=df) # find mean default for each decile
lift = lift[,c(2,4)]
lift[,3] = lift[,2] / mean(holdout_sample$buytabw)
names(lift) = c("decile","Mean Response","Lift Factor")
lift
```
The lift factor gradually increases with each decile, which is good.
